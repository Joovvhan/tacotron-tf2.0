{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import scipy as sp\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tensorflow.keras import Model\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 80\n",
    "fs = 22050\n",
    "frame_length_ms=50\n",
    "frame_shift_ms=12.5\n",
    "nsc = int(22050 * frame_length_ms / 1000)\n",
    "nov = nsc - int(22050 * frame_shift_ms / 1000)\n",
    "nhop = int(22050 * frame_shift_ms / 1000)\n",
    "eps = 1e-10\n",
    "db_ref = 100\n",
    "\n",
    "chars = ' ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz!\\'(),-.:;? '\n",
    "\n",
    "num_tokens = len(chars)\n",
    "\n",
    "embed_size = 256\n",
    "\n",
    "K = 16\n",
    "\n",
    "num_conv1d_filters = 128\n",
    "\n",
    "prenet_size = [256, 128]\n",
    "\n",
    "num_enc_proj_filters = [128, 128]\n",
    "\n",
    "enc_highway_depth = 128\n",
    "\n",
    "enc_bidirection_rnn_depth = 128 # 128 * 2\n",
    "\n",
    "attention_depth = 128\n",
    "\n",
    "dec_rnn_depth = [256, 256]\n",
    "\n",
    "attention_rnn_depth = 256\n",
    "\n",
    "dec_prenet_size = [256, 128]\n",
    "\n",
    "r = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_path = \"../datasets/metadata.csv\"\n",
    "\n",
    "with open(meta_path, encoding='utf-8') as f:\n",
    "    metadata = np.array([line.strip().split('|') for line in f])\n",
    "#     hours = sum((int(x[2]) for x in metadata)) * frame_shift_ms / (3600 * 1000)\n",
    "#     log('Loaded metadata for %d examples (%.2f hours)' % (len(metadata), hours))\n",
    "\n",
    "# metadata = metadata[:32, :2]\n",
    "\n",
    "max_sequence_len = max(list(map(len, metadata[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_name_list = []\n",
    "\n",
    "for data in metadata:\n",
    "    wav_name = '{}.wav'.format(data[0])\n",
    "    wave_name_list.append(wav_name)\n",
    "    \n",
    "data_folder = \"../datasets/wavs\"\n",
    "specgram_folder = \"../datasets/specgrams\"\n",
    "mel_folder = \"../datasets/mels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nPreprocessing Step\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Preprocessing Step\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# for wav_name in tqdm(wave_name_list):\n",
    "#     wav_path = os.path.join(data_folder, wav_name)\n",
    "    \n",
    "#     npy_name = wav_name.replace('.wav', '.npy')\n",
    "    \n",
    "#     specgram_path = os.path.join(specgram_folder, npy_name)\n",
    "#     mel_path = os.path.join(mel_folder, npy_name)\n",
    "    \n",
    "#     y, sr = librosa.core.load(wav_path)\n",
    "    \n",
    "#     f, t, Zxx = sp.signal.stft(y, fs=sr, nperseg=nsc, noverlap=nov)\n",
    "\n",
    "#     Sxx = np.abs(Zxx)\n",
    "#     Sxx = np.maximum(Sxx, eps)\n",
    "\n",
    "#     # plt.figure(figsize=(20,20))\n",
    "#     # plt.imshow(20*np.log10(Sxx), origin='lower')\n",
    "#     # plt.colorbar()\n",
    "#     # plt.show()\n",
    "\n",
    "#     mel_filters = librosa.filters.mel(sr=fs, n_fft=nsc, n_mels=n_mels)\n",
    "\n",
    "#     mel_specgram = np.matmul(mel_filters, Sxx)\n",
    "\n",
    "#     log_specgram = 20*np.log10(Sxx)\n",
    "\n",
    "#     norm_log_specgram = (log_specgram + db_ref) / db_ref\n",
    "\n",
    "#     log_mel_specgram = 20 * np.log10(np.maximum(mel_specgram, eps))\n",
    "\n",
    "#     norm_log_mel_specgram = (log_mel_specgram + db_ref) / db_ref\n",
    "    \n",
    "# #     np.save(specgram_path, norm_log_specgram)\n",
    "# #     np.save(mel_path, norm_log_mel_specgram)\n",
    "#     np.save(specgram_path, Sxx)\n",
    "#     np.save(mel_path, norm_log_mel_specgram)\n",
    "    \n",
    "#     print(norm_log_mel_specgram.shape[1])\n",
    "    \n",
    "\n",
    "#     plt.figure(figsize=(16,9))\n",
    "#     plt.imshow(Sxx, origin='lower', aspect='auto')\n",
    "#     plt.colorbar()\n",
    "#     plt.show()\n",
    "\n",
    "#     plt.figure(figsize=(16,9))\n",
    "#     plt.imshow(norm_log_mel_specgram, origin='lower', aspect='auto')\n",
    "#     plt.colorbar()\n",
    "#     plt.show()    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(num_tokens, embed_size)\n",
    "\n",
    "\n",
    "def initialize_GO_frame(batch_size, n_mels):\n",
    "    return tf.zeros((batch_size, r, n_mels))\n",
    "\n",
    "\n",
    "def flatten_r_frame(input_tensor, batch_size):\n",
    "    return tf.reshape(input_tensor, [batch_size, 1, -1])\n",
    "\n",
    "\n",
    "class Conv1D_Bank(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, num_filters, K):\n",
    "        super(Conv1D_Bank, self).__init__()\n",
    "        \n",
    "        self.K = K\n",
    "        \n",
    "        self.conv1d_filters = [tf.keras.Sequential([tf.keras.layers.Conv1D(kernel_size=k+1, filters=num_filters, padding='same'),\n",
    "                                                   tf.keras.layers.BatchNormalization(),\n",
    "                                                   tf.keras.layers.Activation('relu')])\n",
    "                               for k in range(K)]\n",
    "        \n",
    "    def call(self, input_tensor):\n",
    "        \n",
    "        intermediate_results = []\n",
    "        \n",
    "        for k in range(self.K):\n",
    "            conv_k_result = self.conv1d_filters[k](input_tensor)\n",
    "            intermediate_results.append(conv_k_result)\n",
    "        \n",
    "        output_tensor = tf.concat(intermediate_results, axis = -1) \n",
    "        \n",
    "        return output_tensor\n",
    "    \n",
    "class HighwayNet(tf.keras.Model):\n",
    "    def __init__(self, num_units):\n",
    "        super(HighwayNet, self).__init__()\n",
    "        self.T = tf.keras.layers.Dense(units=num_units, activation='sigmoid',\n",
    "                                        bias_initializer=tf.constant_initializer(-1.0))\n",
    "        self.H = tf.keras.layers.Dense(units=num_units, activation='relu')\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        output_tensor = self.H(input_tensor) * self.T(input_tensor) + input_tensor * (1 - self.T(input_tensor))\n",
    "        return output_tensor\n",
    "    \n",
    "'''\n",
    "https://www.tensorflow.org/beta/tutorials/text/nmt_with_attention\n",
    "\n",
    "'''\n",
    "    \n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(attention_depth)\n",
    "        self.W2 = tf.keras.layers.Dense(attention_depth)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        context_vector = tf.expand_dims(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "class ResidualGRU(tf.keras.Model):\n",
    "    def __init__(self, units, return_sequences, recurrent_initializer='glorot_uniform'):\n",
    "        super(ResidualGRU, self).__init__()\n",
    "        self.units = units\n",
    "        self.gru_layer = tf.keras.layers.GRU(units, return_sequences=True, recurrent_initializer=recurrent_initializer)\n",
    "\n",
    "    def call(self, input_tensor, initial_state):\n",
    "#         print(\"initial_state: {}\".format(initial_state.shape))\n",
    "        output_tensor = self.gru_layer(input_tensor, initial_state = initial_state)\n",
    "        \n",
    "        residual_ouput_tensor = tf.add(output_tensor, input_tensor[:, :, :self.units])\n",
    "        \n",
    "        return residual_ouput_tensor\n",
    "    \n",
    "    \n",
    "class StackedResidualRNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(StackedResidualRNN, self).__init__()\n",
    "        self.depth = dec_rnn_depth\n",
    "        self.residual_grues = [ResidualGRU(dec_rnn_depth[0], return_sequences=True, recurrent_initializer='glorot_uniform'),\n",
    "                               ResidualGRU(dec_rnn_depth[1], return_sequences=True, recurrent_initializer='glorot_uniform')]\n",
    "\n",
    "    def call(self, input_tensor, hidden_states):\n",
    "\n",
    "        for i, residual_gru in enumerate(self.residual_grues):\n",
    "#             print(\"hidden_states[i]: {}\".format(hidden_states[i].shape))\n",
    "            output_tensor = residual_gru(input_tensor, initial_state=hidden_states[i])\n",
    "            hidden_states[i] = tf.reshape(output_tensor, [output_tensor.shape[0], -1])\n",
    "\n",
    "        return output_tensor, hidden_states\n",
    "    \n",
    "    def initialize_hidden_states(self, batch_size):\n",
    "        return [tf.zeros((batch_size, dec_rnn_depth[0])), tf.zeros((batch_size, dec_rnn_depth[1]))]      \n",
    "\n",
    "class AttentionRNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(AttentionRNN, self).__init__()\n",
    "        self.depth = attention_rnn_depth\n",
    "        self.gru_cell = tf.keras.layers.GRU(self.depth, return_sequences=True, recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, input_tensor, initial_state):\n",
    "        output_tensor = self.gru_cell(input_tensor, initial_state=initial_state)\n",
    "        hidden_state = output_tensor\n",
    "        hidden_state = tf.reshape(hidden_state, [hidden_state.shape[0], -1])\n",
    "\n",
    "        return output_tensor, hidden_state\n",
    "    \n",
    "    def initialize_hidden_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.depth))      \n",
    "\n",
    "class DecoderRNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.dec_rnn_depth = dec_rnn_depth\n",
    "        self.att_rnn_depth = attention_rnn_depth\n",
    "        \n",
    "        self.attention_rnn_layer = AttentionRNN()\n",
    "        self.residual_rnn_layers = StackedResidualRNN()\n",
    "        \n",
    "    def call(self, input_tensor, hidden_states, context_vector):\n",
    "        \n",
    "#         for i, state in enumerate(hidden_states):\n",
    "#             print(\"state {}: {}\".format(i, state.shape))\n",
    "        \n",
    "        state, hidden_states[0] = self.attention_rnn_layer(input_tensor, initial_state=hidden_states[0])\n",
    "        \n",
    "#         print('state: {}'.format(state.shape))\n",
    "        \n",
    "        res_rnn_input_tensor = tf.concat([state, context_vector], axis=-1)\n",
    "        \n",
    "        output_tensor, hidden_states[1:3] = self.residual_rnn_layers(res_rnn_input_tensor, hidden_states[1:3])\n",
    "        \n",
    "        return output_tensor, hidden_states\n",
    "    \n",
    "    def initialize_hidden_states(self, batch_size):\n",
    "        hidden_states = [self.attention_rnn_layer.initialize_hidden_state(batch_size)] + self.residual_rnn_layers.initialize_hidden_states(batch_size)\n",
    "#         print(\"hidden_states: {}\".format(len(hidden_states)))\n",
    "#         for i, state in enumerate(hidden_states):\n",
    "#             print(\"state {}: {}\".format(i, state.shape))\n",
    "        return hidden_states\n",
    "    \n",
    "class DecoderPrenet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(DecoderPrenet, self).__init__()\n",
    "        self.decoder_prenet_layer = tf.keras.Sequential([tf.keras.layers.Dense(dec_prenet_size[0]),\n",
    "                         tf.keras.layers.BatchNormalization(),\n",
    "                         tf.keras.layers.Activation('relu'),\n",
    "                         tf.keras.layers.Dropout(0.5),\n",
    "                         tf.keras.layers.Dense(dec_prenet_size[1]),\n",
    "                         tf.keras.layers.BatchNormalization(),\n",
    "                         tf.keras.layers.Activation('relu'),\n",
    "                         tf.keras.layers.Dropout(0.5)])\n",
    "        \n",
    "    def call(self, input_tensor):\n",
    "        output_tensor = self.decoder_prenet_layer(input_tensor)\n",
    "        return output_tensor\n",
    "        \n",
    "class MelPredictor(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MelPredictor, self).__init__()\n",
    "        self.mel_dense_layers = [tf.keras.layers.Dense(n_mels) for i in range(r)]\n",
    "        self.r = r\n",
    "    \n",
    "    def call(self, input_tensor):\n",
    "        \n",
    "        output = []\n",
    "        \n",
    "        for i, dense_layer in enumerate(self.mel_dense_layers):\n",
    "            output.append(dense_layer(input_tensor))\n",
    "            \n",
    "        output_tensor = tf.concat(output, axis=1)\n",
    "        \n",
    "        return output_tensor\n",
    "    \n",
    "class CBHG(tf.keras.Model):\n",
    "    def __init__(self, num_conv1d_filters, K, num_proj_filters, highway_depth, bidirection_rnn_depth):\n",
    "        super(CBHG, self).__init__()\n",
    "        self.conv1d_bank = Conv1D_Bank(num_conv1d_filters, K)\n",
    "        self.max_pooling_layer = tf.keras.layers.MaxPool1D(pool_size=2, strides=1, padding='same')\n",
    "        self.projection_layer = tf.keras.Sequential([tf.keras.layers.Conv1D(kernel_size=3, filters=num_proj_filters[0], padding='same'),\n",
    "                                                   tf.keras.layers.BatchNormalization(),\n",
    "                                                   tf.keras.layers.Activation('relu'),\n",
    "                                                   tf.keras.layers.Conv1D(kernel_size=3, filters=num_proj_filters[1], padding='same'),\n",
    "                                                   tf.keras.layers.BatchNormalization(),\n",
    "                                                   tf.keras.layers.Activation('linear')\n",
    "                                                   ])\n",
    "        \n",
    "        self.highway_preprocess_layer = tf.keras.layers.Dense(highway_depth)\n",
    "        \n",
    "        self.highway_layer = tf.keras.Sequential([HighwayNet(highway_depth) for i in range(4)])\n",
    "        \n",
    "        self.bi_gru_layer = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(bidirection_rnn_depth, return_sequences=True), merge_mode=\"concat\")\n",
    "\n",
    "        \n",
    "    def call(self, input_tensor):\n",
    "        \n",
    "        bank_output = self.conv1d_bank(input_tensor)\n",
    "        max_pool_output = self.max_pooling_layer(bank_output) \n",
    "        projection_output = self.projection_layer(max_pool_output)\n",
    "        \n",
    "        residual_output = tf.add(projection_output, input_tensor)\n",
    "        \n",
    "        highway_intput = self.highway_preprocess_layer(residual_output)\n",
    "        highway_output = self.highway_layer(highway_intput)\n",
    "        output_tensor = self.bi_gru_layer(highway_output)\n",
    "        \n",
    "        return output_tensor\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "class Tacotron():\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Tacotron, self).__init__()\n",
    "        \n",
    "        self.embedding_layer = tf.keras.layers.Embedding(num_tokens, embed_size) \n",
    "\n",
    "        self.encoder_prenet_layer = tf.keras.Sequential([tf.keras.layers.Dense(prenet_size[0], input_shape=(None, embed_size)),\n",
    "                                 tf.keras.layers.BatchNormalization(),\n",
    "                                 tf.keras.layers.Activation('relu'),\n",
    "                                 tf.keras.layers.Dropout(0.5),\n",
    "                                 tf.keras.layers.Dense(prenet_size[1]),\n",
    "                                 tf.keras.layers.BatchNormalization(),\n",
    "                                 tf.keras.layers.Activation('relu'),\n",
    "                                 tf.keras.layers.Dropout(0.5)], name='Encoder_Prenet')\n",
    "    \n",
    "        self.encoder_conv_bank_layer = Conv1D_Bank(num_conv1d_filters, K)\n",
    "        \n",
    "        self.enc_max_pooling_layer = tf.keras.layers.MaxPool1D(pool_size=2, strides=1, padding='same')\n",
    "        \n",
    "        self.enc_proj_layer = tf.keras.Sequential([tf.keras.layers.Conv1D(kernel_size=3, filters=num_enc_proj_filters[0], padding='same'),\n",
    "                                                   tf.keras.layers.BatchNormalization(),\n",
    "                                                   tf.keras.layers.Activation('relu'),\n",
    "                                                   tf.keras.layers.Conv1D(kernel_size=3, filters=num_enc_proj_filters[1], padding='same'),\n",
    "                                                   tf.keras.layers.BatchNormalization(),\n",
    "                                                   tf.keras.layers.Activation('linear')\n",
    "                                                   ])\n",
    "        \n",
    "        self.enc_highway_layer = tf.keras.Sequential([HighwayNet(128) for i in range(4)])\n",
    "    \n",
    "        self.enc_bi_rnn_layer = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(enc_bidirection_rnn_depth, return_sequences=True), merge_mode=\"concat\")\n",
    "\n",
    "        self.attention_layer = BahdanauAttention()\n",
    "        \n",
    "        self.decoder_rnn_layer = DecoderRNN()\n",
    "        \n",
    "        self.decoder_prenet_layer = DecoderPrenet()\n",
    "        \n",
    "        self.mel_pred_layer = MelPredictor()\n",
    "        \n",
    "        self.post_processing_cbhg = CBHG(128, 8, (256, 80), 128, 128)\n",
    "        \n",
    "        self.final_dense_layer = tf.keras.layers.Dense(552)\n",
    "        \n",
    "        self.L1 = tf.keras.losses.MeanAbsoluteError()\n",
    "        \n",
    "#         self.L2 = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "     \n",
    "#     @tf.function\n",
    "    def train(self, mel, linear, input_tensor):\n",
    "        \n",
    "        batch_size = input_tensor.shape[0]\n",
    "        sequence_length = input_tensor.shape[1]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            embedded = self.embedding_layer(input_tensor)\n",
    "            encoded = self.encoder_prenet_layer(embedded)\n",
    "            banked = self.encoder_conv_bank_layer(encoded)\n",
    "            max_pooled = self.enc_max_pooling_layer(banked)\n",
    "            projected = self.enc_proj_layer(max_pooled)\n",
    "            residual_output = tf.add(encoded, projected)\n",
    "            highway_output = self.enc_highway_layer(residual_output)\n",
    "            encoder_output = self.enc_bi_rnn_layer(highway_output)\n",
    "            \n",
    "            hidden_state = encoder_output[:, -1, :]\n",
    "            hidden_states = self.decoder_rnn_layer.initialize_hidden_states(batch_size)\n",
    "            hidden_states[0] = hidden_state\n",
    "\n",
    "            frames = []\n",
    "            frame = initialize_GO_frame(batch_size, n_mels)\n",
    "\n",
    "            for i in range(sequence_length):\n",
    "                flat_frame = flatten_r_frame(frame, batch_size)\n",
    "    #             print(\"Loop {}, flat_frame: {}\".format(i, flat_frame.shape))\n",
    "                decoder_rnn_input = self.decoder_prenet_layer(flat_frame)\n",
    "    #             print(\"Loop {}, decoder_rnn_input: {}\".format(i, decoder_rnn_input.shape))\n",
    "                context_vector, _ = self.attention_layer(hidden_states[0], encoder_output)\n",
    "\n",
    "                decoder_rnn_output, hidden_states = self.decoder_rnn_layer(decoder_rnn_input, hidden_states, context_vector)\n",
    "\n",
    "                mel_frame = self.mel_pred_layer(decoder_rnn_output)\n",
    "                \n",
    "                frames.append(mel_frame)\n",
    "\n",
    "                # No force teaching\n",
    "                frame = mel_frame\n",
    "\n",
    "                # Force teaching\n",
    "\n",
    "            mel_pred = tf.concat(frames, axis=1)\n",
    "            post_cbhg_output = self.post_processing_cbhg(mel_pred)\n",
    "            linear_pred = self.final_dense_layer(post_cbhg_output)\n",
    "\n",
    "            mel_loss = self.L1(mel[:, :mel_pred.shape[1], :], mel_pred)\n",
    "            linear_loss = self.L1(linear[:, :linear_pred.shape[1], :], linear_pred)\n",
    "            total_loss = mel_loss + linear_loss\n",
    "            \n",
    "            variables = self.embedding_layer.trainable_variables + self.encoder_prenet_layer.trainable_variables + \\\n",
    "            self.encoder_conv_bank_layer.trainable_variables + self.enc_max_pooling_layer.trainable_variables + \\\n",
    "            self.enc_proj_layer.trainable_variables + self.enc_highway_layer.trainable_variables + \\\n",
    "            self.enc_bi_rnn_layer.trainable_variables + self.attention_layer.trainable_variables + \\\n",
    "            self.decoder_rnn_layer.trainable_variables + self.decoder_prenet_layer.trainable_variables + \\\n",
    "            self.mel_pred_layer.trainable_variables + self.post_processing_cbhg.trainable_variables + \\\n",
    "            self.final_dense_layer.trainable_variables\n",
    "            \n",
    "            gradients = tape.gradient(total_loss, variables)\n",
    "            \n",
    "            self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        return total_loss\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', lower=False, char_level=True)\n",
    "tokenizer.fit_on_texts(chars)\n",
    "# tokenizer.index_word\n",
    "\n",
    "tokenized_texts = tokenizer.texts_to_sequences(metadata[:, 1])\n",
    "\n",
    "def dataset_generator():\n",
    "    for file_name, tokens in zip(metadata[:, 0], tokenized_texts):\n",
    "#         print(file_name.dtype)\n",
    "        linear_path = os.path.join(specgram_folder, file_name + '.npy')\n",
    "        mel_path = os.path.join(mel_folder, file_name + '.npy')\n",
    "        mel_target = np.load(mel_path)\n",
    "        linear_target = np.load(linear_path)\n",
    "        yield mel_target.T, linear_target.T, tokens\n",
    "#         yield file_name, tokens\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(dataset_generator, (tf.float32, tf.float32, tf.int32), output_shapes=(tf.TensorShape([None, n_mels]), tf.TensorShape([None, 552]),tf.TensorShape([None, ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def element_length_function(mel, linear, tokens):\n",
    "#     print(tokens)\n",
    "#     print(file_name)\n",
    "    key = tf.size(tokens)\n",
    "    return key\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "sequence_lengths = [i for i in range(4, max_sequence_len, 4)]\n",
    "batch_sizes = [batch_size for i in range(len(sequence_lengths) + 1)]\n",
    "\n",
    "dataset = dataset.apply(tf.data.experimental.bucket_by_sequence_length(element_length_function, sequence_lengths, batch_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358f1e99e65a44ff8ac443f12ade7aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0709 11:13:47.457193 29996 optimizer_v2.py:979] Gradients does not exist for variables ['decoder_rnn_12/stacked_residual_rnn_12/residual_gru_24/gru_62/kernel:0', 'decoder_rnn_12/stacked_residual_rnn_12/residual_gru_24/gru_62/recurrent_kernel:0', 'decoder_rnn_12/stacked_residual_rnn_12/residual_gru_24/gru_62/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.14932996, shape=(), dtype=float32)\n",
      "(32, 91)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0709 11:13:51.867055 29996 optimizer_v2.py:979] Gradients does not exist for variables ['decoder_rnn_12/stacked_residual_rnn_12/residual_gru_24/gru_62/kernel:0', 'decoder_rnn_12/stacked_residual_rnn_12/residual_gru_24/gru_62/recurrent_kernel:0', 'decoder_rnn_12/stacked_residual_rnn_12/residual_gru_24/gru_62/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.15512186, shape=(), dtype=float32)\n",
      "(32, 131)\n"
     ]
    }
   ],
   "source": [
    "tacotron = Tacotron()\n",
    "\n",
    "for i, (mel, linear, texts) in enumerate(tqdm((dataset))):\n",
    "    print(texts.shape)\n",
    "    x = tacotron.train(mel, linear, texts)\n",
    "    print(x)\n",
    "    if i == 10:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
