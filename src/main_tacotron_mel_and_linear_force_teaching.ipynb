{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import scipy as sp\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tensorflow.keras import Model\n",
    "import unicodedata\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 80\n",
    "fs = 22050\n",
    "frame_length_ms=50\n",
    "frame_shift_ms=12.5\n",
    "nsc = int(22050 * frame_length_ms / 1000)\n",
    "nov = nsc - int(22050 * frame_shift_ms / 1000)\n",
    "nhop = int(22050 * frame_shift_ms / 1000)\n",
    "eps = 1e-10\n",
    "db_ref = 100\n",
    "\n",
    "chars = ' ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz!\\'(),-.:;? '\n",
    "\n",
    "num_tokens = len(chars)\n",
    "\n",
    "embed_size = 256\n",
    "\n",
    "K = 16\n",
    "\n",
    "num_conv1d_filters = 128\n",
    "\n",
    "prenet_size = [256, 128]\n",
    "\n",
    "num_enc_proj_filters = [128, 128]\n",
    "\n",
    "enc_highway_depth = 128\n",
    "\n",
    "enc_bidirection_rnn_depth = 128 # 128 * 2\n",
    "\n",
    "attention_depth = 128\n",
    "\n",
    "dec_rnn_depth = [256, 256]\n",
    "\n",
    "attention_rnn_depth = 256\n",
    "\n",
    "dec_prenet_size = [256, 128]\n",
    "\n",
    "r = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_path = \"../datasets/metadata.csv\"\n",
    "\n",
    "with open(meta_path, encoding='utf-8') as f:\n",
    "    metadata = np.array([line.strip().split('|') for line in f])\n",
    "#     hours = sum((int(x[2]) for x in metadata)) * frame_shift_ms / (3600 * 1000)\n",
    "#     log('Loaded metadata for %d examples (%.2f hours)' % (len(metadata), hours))\n",
    "\n",
    "# metadata = metadata[:32, :2]\n",
    "\n",
    "max_sequence_len = max(list(map(len, metadata[:, 1])))\n",
    "\n",
    "dataset_size = len(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_name_list = []\n",
    "\n",
    "for data in metadata:\n",
    "    wav_name = '{}.wav'.format(data[0])\n",
    "    wave_name_list.append(wav_name)\n",
    "    \n",
    "data_folder = \"../datasets/wavs\"\n",
    "specgram_folder = \"../datasets/specgrams\"\n",
    "mel_folder = \"../datasets/mels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nPreprocessing Step\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Preprocessing Step\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# for wav_name in tqdm(wave_name_list):\n",
    "#     wav_path = os.path.join(data_folder, wav_name)\n",
    "    \n",
    "#     npy_name = wav_name.replace('.wav', '.npy')\n",
    "    \n",
    "#     specgram_path = os.path.join(specgram_folder, npy_name)\n",
    "#     mel_path = os.path.join(mel_folder, npy_name)\n",
    "    \n",
    "#     y, sr = librosa.core.load(wav_path)\n",
    "    \n",
    "#     f, t, Zxx = sp.signal.stft(y, fs=sr, nperseg=nsc, noverlap=nov)\n",
    "\n",
    "#     Sxx = np.abs(Zxx)\n",
    "#     Sxx = np.maximum(Sxx, eps)\n",
    "\n",
    "#     # plt.figure(figsize=(20,20))\n",
    "#     # plt.imshow(20*np.log10(Sxx), origin='lower')\n",
    "#     # plt.colorbar()\n",
    "#     # plt.show()\n",
    "\n",
    "#     mel_filters = librosa.filters.mel(sr=fs, n_fft=nsc, n_mels=n_mels)\n",
    "\n",
    "#     mel_specgram = np.matmul(mel_filters, Sxx)\n",
    "\n",
    "#     log_specgram = 20*np.log10(Sxx)\n",
    "\n",
    "#     norm_log_specgram = (log_specgram + db_ref) / db_ref\n",
    "\n",
    "#     log_mel_specgram = 20 * np.log10(np.maximum(mel_specgram, eps))\n",
    "\n",
    "#     norm_log_mel_specgram = (log_mel_specgram + db_ref) / db_ref\n",
    "    \n",
    "# #     np.save(specgram_path, norm_log_specgram)\n",
    "# #     np.save(mel_path, norm_log_mel_specgram)\n",
    "#     np.save(specgram_path, Sxx)\n",
    "#     np.save(mel_path, norm_log_mel_specgram)\n",
    "    \n",
    "#     print(norm_log_mel_specgram.shape[1])\n",
    "    \n",
    "\n",
    "#     plt.figure(figsize=(16,9))\n",
    "#     plt.imshow(Sxx, origin='lower', aspect='auto')\n",
    "#     plt.colorbar()\n",
    "#     plt.show()\n",
    "\n",
    "#     plt.figure(figsize=(16,9))\n",
    "#     plt.imshow(norm_log_mel_specgram, origin='lower', aspect='auto')\n",
    "#     plt.colorbar()\n",
    "#     plt.show()    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(num_tokens, embed_size)\n",
    "\n",
    "\n",
    "def initialize_GO_frame(batch_size, n_mels):\n",
    "    return tf.zeros((batch_size, r, n_mels))\n",
    "\n",
    "\n",
    "def flatten_r_frame(input_tensor, batch_size):\n",
    "    return tf.reshape(input_tensor, [batch_size, 1, -1])\n",
    "\n",
    "\n",
    "class Conv1D_Bank(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, num_filters, K):\n",
    "        super(Conv1D_Bank, self).__init__()\n",
    "        \n",
    "        self.K = K\n",
    "        \n",
    "        self.conv1d_filters = [tf.keras.Sequential([tf.keras.layers.Conv1D(kernel_size=k+1, filters=num_filters, padding='same'),\n",
    "                                                   tf.keras.layers.BatchNormalization(),\n",
    "                                                   tf.keras.layers.Activation('relu')])\n",
    "                               for k in range(K)]\n",
    "        \n",
    "    def call(self, input_tensor):\n",
    "        \n",
    "        intermediate_results = []\n",
    "        \n",
    "        for k in range(self.K):\n",
    "            conv_k_result = self.conv1d_filters[k](input_tensor)\n",
    "            intermediate_results.append(conv_k_result)\n",
    "        \n",
    "        output_tensor = tf.concat(intermediate_results, axis = -1) \n",
    "        \n",
    "        return output_tensor\n",
    "    \n",
    "class HighwayNet(tf.keras.Model):\n",
    "    def __init__(self, num_units):\n",
    "        super(HighwayNet, self).__init__()\n",
    "        self.T = tf.keras.layers.Dense(units=num_units, activation='sigmoid',\n",
    "                                        bias_initializer=tf.constant_initializer(-1.0))\n",
    "        self.H = tf.keras.layers.Dense(units=num_units, activation='relu')\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        output_tensor = self.H(input_tensor) * self.T(input_tensor) + input_tensor * (1 - self.T(input_tensor))\n",
    "        return output_tensor\n",
    "    \n",
    "'''\n",
    "https://www.tensorflow.org/beta/tutorials/text/nmt_with_attention\n",
    "\n",
    "'''\n",
    "    \n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(attention_depth)\n",
    "        self.W2 = tf.keras.layers.Dense(attention_depth)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        context_vector = tf.expand_dims(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "class ResidualGRU(tf.keras.Model):\n",
    "    def __init__(self, units, return_sequences, recurrent_initializer='glorot_uniform'):\n",
    "        super(ResidualGRU, self).__init__()\n",
    "        self.units = units\n",
    "        self.gru_layer = tf.keras.layers.GRU(units, return_sequences=True, recurrent_initializer=recurrent_initializer)\n",
    "\n",
    "    def call(self, input_tensor, initial_state):\n",
    "        output_tensor = self.gru_layer(input_tensor, initial_state = initial_state)\n",
    "        \n",
    "        residual_ouput_tensor = tf.add(output_tensor, input_tensor[:, :, :self.units])\n",
    "        \n",
    "        return residual_ouput_tensor\n",
    "    \n",
    "    \n",
    "class StackedResidualRNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(StackedResidualRNN, self).__init__()\n",
    "        self.depth = dec_rnn_depth\n",
    "        self.residual_grues = [ResidualGRU(dec_rnn_depth[0], return_sequences=True, recurrent_initializer='glorot_uniform'),\n",
    "                               ResidualGRU(dec_rnn_depth[1], return_sequences=True, recurrent_initializer='glorot_uniform')]\n",
    "\n",
    "    def call(self, input_tensor, hidden_states):\n",
    "\n",
    "        for i, residual_gru in enumerate(self.residual_grues):\n",
    "            output_tensor = residual_gru(input_tensor, initial_state=hidden_states[i])\n",
    "            hidden_states[i] = tf.reshape(output_tensor, [output_tensor.shape[0], -1])\n",
    "\n",
    "        return output_tensor, hidden_states\n",
    "    \n",
    "    def initialize_hidden_states(self, batch_size):\n",
    "        return [tf.zeros((batch_size, dec_rnn_depth[0])), tf.zeros((batch_size, dec_rnn_depth[1]))]      \n",
    "\n",
    "class AttentionRNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(AttentionRNN, self).__init__()\n",
    "        self.depth = attention_rnn_depth\n",
    "        self.gru_cell = tf.keras.layers.GRU(self.depth, return_sequences=True, recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, input_tensor, initial_state):\n",
    "        output_tensor = self.gru_cell(input_tensor, initial_state=initial_state)\n",
    "        hidden_state = output_tensor\n",
    "        hidden_state = tf.reshape(hidden_state, [hidden_state.shape[0], -1])\n",
    "\n",
    "        return output_tensor, hidden_state\n",
    "    \n",
    "    def initialize_hidden_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.depth))      \n",
    "\n",
    "class DecoderRNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.dec_rnn_depth = dec_rnn_depth\n",
    "        self.att_rnn_depth = attention_rnn_depth\n",
    "        \n",
    "        self.attention_rnn_layer = AttentionRNN()\n",
    "        self.residual_rnn_layers = StackedResidualRNN()\n",
    "        \n",
    "    def call(self, input_tensor, hidden_states, context_vector):\n",
    "                \n",
    "        state, hidden_states[0] = self.attention_rnn_layer(input_tensor, initial_state=hidden_states[0])\n",
    "        \n",
    "        res_rnn_input_tensor = tf.concat([state, context_vector], axis=-1)\n",
    "        \n",
    "        output_tensor, hidden_states[1:3] = self.residual_rnn_layers(res_rnn_input_tensor, hidden_states[1:3])\n",
    "        \n",
    "        return output_tensor, hidden_states\n",
    "    \n",
    "    def initialize_hidden_states(self, batch_size):\n",
    "        hidden_states = [self.attention_rnn_layer.initialize_hidden_state(batch_size)] + self.residual_rnn_layers.initialize_hidden_states(batch_size)\n",
    "        return hidden_states\n",
    "    \n",
    "class DecoderPrenet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(DecoderPrenet, self).__init__()\n",
    "        self.decoder_prenet_layer = tf.keras.Sequential([tf.keras.layers.Dense(dec_prenet_size[0]),\n",
    "                         tf.keras.layers.BatchNormalization(),\n",
    "                         tf.keras.layers.Activation('relu'),\n",
    "                         tf.keras.layers.Dropout(0.5),\n",
    "                         tf.keras.layers.Dense(dec_prenet_size[1]),\n",
    "                         tf.keras.layers.BatchNormalization(),\n",
    "                         tf.keras.layers.Activation('relu'),\n",
    "                         tf.keras.layers.Dropout(0.5)])\n",
    "        \n",
    "    def call(self, input_tensor):\n",
    "        output_tensor = self.decoder_prenet_layer(input_tensor)\n",
    "        return output_tensor\n",
    "        \n",
    "class MelPredictor(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MelPredictor, self).__init__()\n",
    "        self.mel_dense_layers = [tf.keras.layers.Dense(n_mels) for i in range(r)]\n",
    "        self.r = r\n",
    "    \n",
    "    def call(self, input_tensor):\n",
    "        \n",
    "        output = []\n",
    "        \n",
    "        for i, dense_layer in enumerate(self.mel_dense_layers):\n",
    "            output.append(dense_layer(input_tensor))\n",
    "            \n",
    "        output_tensor = tf.concat(output, axis=1)\n",
    "        \n",
    "        return output_tensor\n",
    "    \n",
    "class CBHG(tf.keras.Model):\n",
    "    def __init__(self, num_conv1d_filters, K, num_proj_filters, highway_depth, bidirection_rnn_depth):\n",
    "        super(CBHG, self).__init__()\n",
    "        self.conv1d_bank = Conv1D_Bank(num_conv1d_filters, K)\n",
    "        self.max_pooling_layer = tf.keras.layers.MaxPool1D(pool_size=2, strides=1, padding='same')\n",
    "        self.projection_layer = tf.keras.Sequential([tf.keras.layers.Conv1D(kernel_size=3, filters=num_proj_filters[0], padding='same'),\n",
    "                                                   tf.keras.layers.BatchNormalization(),\n",
    "                                                   tf.keras.layers.Activation('relu'),\n",
    "                                                   tf.keras.layers.Conv1D(kernel_size=3, filters=num_proj_filters[1], padding='same'),\n",
    "                                                   tf.keras.layers.BatchNormalization(),\n",
    "                                                   tf.keras.layers.Activation('linear')\n",
    "                                                   ])\n",
    "        \n",
    "        self.highway_preprocess_layer = tf.keras.layers.Dense(highway_depth)\n",
    "        \n",
    "        self.highway_layer = tf.keras.Sequential([HighwayNet(highway_depth) for i in range(4)])\n",
    "        \n",
    "        self.bi_gru_layer = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(bidirection_rnn_depth, return_sequences=True), merge_mode=\"concat\")\n",
    "\n",
    "        \n",
    "    def call(self, input_tensor):\n",
    "        \n",
    "        bank_output = self.conv1d_bank(input_tensor)\n",
    "        max_pool_output = self.max_pooling_layer(bank_output) \n",
    "        projection_output = self.projection_layer(max_pool_output)\n",
    "        \n",
    "        residual_output = tf.add(projection_output, input_tensor)\n",
    "        \n",
    "        highway_intput = self.highway_preprocess_layer(residual_output)\n",
    "        highway_output = self.highway_layer(highway_intput)\n",
    "        output_tensor = self.bi_gru_layer(highway_output)\n",
    "        \n",
    "        return output_tensor\n",
    "        \n",
    "\n",
    "class Tacotron():\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Tacotron, self).__init__()\n",
    "        \n",
    "        self.embedding_layer = tf.keras.layers.Embedding(num_tokens, embed_size) \n",
    "\n",
    "        self.encoder_prenet_layer = tf.keras.Sequential([tf.keras.layers.Dense(prenet_size[0], input_shape=(None, embed_size)),\n",
    "                                 tf.keras.layers.BatchNormalization(),\n",
    "                                 tf.keras.layers.Activation('relu'),\n",
    "                                 tf.keras.layers.Dropout(0.5),\n",
    "                                 tf.keras.layers.Dense(prenet_size[1]),\n",
    "                                 tf.keras.layers.BatchNormalization(),\n",
    "                                 tf.keras.layers.Activation('relu'),\n",
    "                                 tf.keras.layers.Dropout(0.5)], name='Encoder_Prenet')\n",
    "    \n",
    "        self.encoder_conv_bank_layer = Conv1D_Bank(num_conv1d_filters, K)\n",
    "        \n",
    "        self.enc_max_pooling_layer = tf.keras.layers.MaxPool1D(pool_size=2, strides=1, padding='same')\n",
    "        \n",
    "        self.enc_proj_layer = tf.keras.Sequential([tf.keras.layers.Conv1D(kernel_size=3, filters=num_enc_proj_filters[0], padding='same'),\n",
    "                                                   tf.keras.layers.BatchNormalization(),\n",
    "                                                   tf.keras.layers.Activation('relu'),\n",
    "                                                   tf.keras.layers.Conv1D(kernel_size=3, filters=num_enc_proj_filters[1], padding='same'),\n",
    "                                                   tf.keras.layers.BatchNormalization(),\n",
    "                                                   tf.keras.layers.Activation('linear')\n",
    "                                                   ])\n",
    "        \n",
    "        self.enc_highway_layer = tf.keras.Sequential([HighwayNet(128) for i in range(4)])\n",
    "    \n",
    "        self.enc_bi_rnn_layer = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(enc_bidirection_rnn_depth, return_sequences=True), merge_mode=\"concat\")\n",
    "\n",
    "        self.attention_layer = BahdanauAttention()\n",
    "        \n",
    "        self.decoder_rnn_layer = DecoderRNN()\n",
    "        \n",
    "        self.decoder_prenet_layer = DecoderPrenet()\n",
    "        \n",
    "        self.mel_pred_layer = MelPredictor()\n",
    "        \n",
    "        self.post_processing_cbhg = CBHG(128, 8, (256, 80), 128, 128)\n",
    "        \n",
    "        self.final_dense_layer = tf.keras.layers.Dense(552)\n",
    "        \n",
    "        self.L1 = tf.keras.losses.MeanAbsoluteError()\n",
    "        \n",
    "#         self.L2 = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "     \n",
    "    @tf.function\n",
    "    def train(self, mel, linear, input_tensor):\n",
    "        \n",
    "        batch_size = input_tensor.shape[0]\n",
    "        sequence_length = input_tensor.shape[1]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            embedded = self.embedding_layer(input_tensor)\n",
    "            encoded = self.encoder_prenet_layer(embedded)\n",
    "            banked = self.encoder_conv_bank_layer(encoded)\n",
    "            max_pooled = self.enc_max_pooling_layer(banked)\n",
    "            projected = self.enc_proj_layer(max_pooled)\n",
    "            residual_output = tf.add(encoded, projected)\n",
    "            highway_output = self.enc_highway_layer(residual_output)\n",
    "            encoder_output = self.enc_bi_rnn_layer(highway_output)\n",
    "            \n",
    "            hidden_state = encoder_output[:, -1, :]\n",
    "            hidden_states = self.decoder_rnn_layer.initialize_hidden_states(batch_size)\n",
    "            hidden_states[0] = hidden_state\n",
    "\n",
    "            frames = []\n",
    "            frame = initialize_GO_frame(batch_size, n_mels)\n",
    "\n",
    "            for i in range(sequence_length):\n",
    "                flat_frame = flatten_r_frame(frame, batch_size)\n",
    "                decoder_rnn_input = self.decoder_prenet_layer(flat_frame)\n",
    "                context_vector, _ = self.attention_layer(hidden_states[0], encoder_output)\n",
    "                decoder_rnn_output, hidden_states = self.decoder_rnn_layer(decoder_rnn_input, hidden_states, context_vector)\n",
    "\n",
    "                mel_frame = self.mel_pred_layer(decoder_rnn_output)\n",
    "                \n",
    "                frames.append(mel_frame)\n",
    "                \n",
    "                #frame = mel_frame #No force teaching\n",
    "                \n",
    "                frame = mel[:, r*(i):r*(i+1), :] # Force teaching\n",
    "                \n",
    "            \n",
    "            mel_pred = tf.concat(frames, axis=1)\n",
    "            post_cbhg_output = self.post_processing_cbhg(mel_pred)\n",
    "            linear_pred = self.final_dense_layer(post_cbhg_output)\n",
    "\n",
    "            mel_loss = self.L1(mel[:, :mel_pred.shape[1], :], mel_pred)\n",
    "            linear_loss = self.L1(linear[:, :linear_pred.shape[1], :], linear_pred)\n",
    "            total_loss = mel_loss + linear_loss\n",
    "            \n",
    "            print(\"Loss Calculated\")\n",
    "            print(time.strftime(\"%Hh %Mm %Ss\"))\n",
    "            \n",
    "            variables = self.embedding_layer.trainable_variables + self.encoder_prenet_layer.trainable_variables + \\\n",
    "            self.encoder_conv_bank_layer.trainable_variables + self.enc_max_pooling_layer.trainable_variables + \\\n",
    "            self.enc_proj_layer.trainable_variables + self.enc_highway_layer.trainable_variables + \\\n",
    "            self.enc_bi_rnn_layer.trainable_variables + self.attention_layer.trainable_variables + \\\n",
    "            self.decoder_rnn_layer.trainable_variables + self.decoder_prenet_layer.trainable_variables + \\\n",
    "            self.mel_pred_layer.trainable_variables + self.post_processing_cbhg.trainable_variables + \\\n",
    "            self.final_dense_layer.trainable_variables\n",
    "            \n",
    "            gradients = tape.gradient(total_loss, variables)\n",
    "            \n",
    "            self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        return total_loss / (r * sequence_length)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', lower=False, char_level=True)\n",
    "tokenizer.fit_on_texts(chars)\n",
    "\n",
    "tokenized_texts = tokenizer.texts_to_sequences(metadata[:, 1])\n",
    "\n",
    "def dataset_generator():\n",
    "    for file_name, tokens in zip(metadata[:, 0], tokenized_texts):\n",
    "        linear_path = os.path.join(specgram_folder, file_name + '.npy')\n",
    "        mel_path = os.path.join(mel_folder, file_name + '.npy')\n",
    "        mel_target = np.load(mel_path)\n",
    "        linear_target = np.load(linear_path)\n",
    "        yield mel_target.T, linear_target.T, tokens\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0709 18:51:05.160028 37052 deprecation.py:323] From c:\\users\\jw\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:505: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_generator(dataset_generator, (tf.float32, tf.float32, tf.int32), output_shapes=(tf.TensorShape([None, n_mels]), tf.TensorShape([None, 552]),tf.TensorShape([None, ])))\n",
    "\n",
    "# dataset.shuffle(dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0709 18:51:05.194036 37052 deprecation.py:323] From c:\\users\\jw\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\grouping.py:193: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "def element_length_function(mel, linear, tokens):\n",
    "    key = tf.size(tokens)\n",
    "    return key\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "sequence_lengths = [i + 1 for i in range(16, max_sequence_len, 16)]\n",
    "batch_sizes = [batch_size for i in range(len(sequence_lengths) + 1)]\n",
    "\n",
    "dataset = dataset.apply(tf.data.experimental.bucket_by_sequence_length(element_length_function, sequence_lengths, batch_sizes))\n",
    "dataset = dataset.prefetch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18h 51m 05s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7600aeaa664a482d853ecac6a1efbf89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 110)\n",
      "Loss Calculated\n",
      "18h 53m 46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0709 18:55:10.048389 37052 optimizer_v2.py:979] Gradients does not exist for variables ['decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/kernel:0', 'decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/recurrent_kernel:0', 'decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Calculated\n",
      "18h 57m 51s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0709 18:59:12.263768 37052 optimizer_v2.py:979] Gradients does not exist for variables ['decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/kernel:0', 'decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/recurrent_kernel:0', 'decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.000664999, shape=(), dtype=float32)\n",
      "Time taken for an epoch: 600.4 sec\n",
      "19h 01m 07s\n",
      "\n",
      "(16, 80)\n",
      "Loss Calculated\n",
      "19h 02m 59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0709 19:04:01.915571 37052 optimizer_v2.py:979] Gradients does not exist for variables ['decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/kernel:0', 'decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/recurrent_kernel:0', 'decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0008096206, shape=(), dtype=float32)\n",
      "Time taken for an epoch: 253.4 sec\n",
      "19h 05m 20s\n",
      "\n",
      "(16, 95)\n",
      "Loss Calculated\n",
      "19h 07m 47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0709 19:09:05.193228 37052 optimizer_v2.py:979] Gradients does not exist for variables ['decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/kernel:0', 'decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/recurrent_kernel:0', 'decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0005638629, shape=(), dtype=float32)\n",
      "Time taken for an epoch: 319.8 sec\n",
      "19h 10m 40s\n",
      "\n",
      "(16, 144)\n",
      "Loss Calculated\n",
      "19h 14m 15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0709 19:16:10.442417 37052 optimizer_v2.py:979] Gradients does not exist for variables ['decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/kernel:0', 'decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/recurrent_kernel:0', 'decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.00034945476, shape=(), dtype=float32)\n",
      "Time taken for an epoch: 482.6 sec\n",
      "19h 18m 42s\n",
      "\n",
      "(16, 127)\n",
      "Loss Calculated\n",
      "19h 21m 51s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0709 19:23:20.184913 37052 optimizer_v2.py:979] Gradients does not exist for variables ['decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/kernel:0', 'decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/recurrent_kernel:0', 'decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.00040202457, shape=(), dtype=float32)\n",
      "Time taken for an epoch: 415.5 sec\n",
      "19h 25m 38s\n",
      "\n",
      "(16, 64)\n",
      "Loss Calculated\n",
      "19h 27m 08s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0709 19:27:54.308448 37052 optimizer_v2.py:979] Gradients does not exist for variables ['decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/kernel:0', 'decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/recurrent_kernel:0', 'decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0007365497, shape=(), dtype=float32)\n",
      "Time taken for an epoch: 202.7 sec\n",
      "19h 29m 01s\n",
      "\n",
      "(16, 109)\n",
      "Loss Calculated\n",
      "19h 31m 55s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0709 19:33:33.389854 37052 optimizer_v2.py:979] Gradients does not exist for variables ['decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/kernel:0', 'decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/recurrent_kernel:0', 'decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.00044029756, shape=(), dtype=float32)\n",
      "Time taken for an epoch: 395.2 sec\n",
      "19h 35m 36s\n",
      "\n",
      "(16, 92)\n",
      "Loss Calculated\n",
      "19h 39m 05s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0709 19:40:18.518477 37052 optimizer_v2.py:979] Gradients does not exist for variables ['decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/kernel:0', 'decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/recurrent_kernel:0', 'decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.00049992226, shape=(), dtype=float32)\n",
      "Time taken for an epoch: 391.7 sec\n",
      "19h 42m 08s\n",
      "\n",
      "(16, 78)\n",
      "Loss Calculated\n",
      "19h 43m 59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0709 19:44:57.061835 37052 optimizer_v2.py:979] Gradients does not exist for variables ['decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/kernel:0', 'decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/recurrent_kernel:0', 'decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0005889737, shape=(), dtype=float32)\n",
      "Time taken for an epoch: 258.8 sec\n",
      "19h 46m 27s\n",
      "\n",
      "(16, 128)\n",
      "Loss Calculated\n",
      "19h 52m 46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0709 19:54:54.472352 37052 optimizer_v2.py:979] Gradients does not exist for variables ['decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/kernel:0', 'decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/recurrent_kernel:0', 'decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0003479824, shape=(), dtype=float32)\n",
      "Time taken for an epoch: 678.9 sec\n",
      "19h 57m 45s\n",
      "\n",
      "(16, 143)\n",
      "Loss Calculated\n",
      "20h 01m 06s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0709 20:02:50.552983 37052 optimizer_v2.py:979] Gradients does not exist for variables ['decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/kernel:0', 'decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/recurrent_kernel:0', 'decoder_rnn/stacked_residual_rnn/residual_gru/gru_2/bias:0'] when minimizing the loss.\n"
     ]
    }
   ],
   "source": [
    "tacotron = Tacotron()\n",
    "\n",
    "# Started hour\n",
    "print(time.strftime(\"%Hh %Mm %Ss\"))\n",
    "\n",
    "for i, (mel, linear, texts) in enumerate(tqdm((dataset))):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    print(texts.shape)    \n",
    "    loss = tacotron.train(mel, linear, texts)\n",
    "    print(loss)\n",
    "    print('Time taken for an epoch: {:.1f} sec'.format(time.time() - start))\n",
    "    print(time.strftime(\"%Hh %Mm %Ss\"))\n",
    "    print('')\n",
    "    \n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
